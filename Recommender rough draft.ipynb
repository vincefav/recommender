{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Favorite Things\n",
    "#### A Tour of Unstructured Data\n",
    "\n",
    " \n",
    "I scraped the profiles of several thousand users on a popular website. Among the data on each profile is their favorite books, movies, TV shows, and even food. But this data is hard to make sense of; it's just a list of words.\n",
    "\n",
    "Using some natural language processing, however, we can tame this data and discover some interesting insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation, digits\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (7116, 2)\n",
      "Columns: Index(['favorites', 'length'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorites</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I read about anything! At the momentI am readi...</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>Books: anything by Mochael Crichton, Jules Ver...</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>I have read like 50 books in my life time that...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>Drunk History, Shameless, Everything is Illumi...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>Shows: Prison BreakMusic: RNB, KpopFood: Spagh...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              favorites  length\n",
       "3     I read about anything! At the momentI am readi...     280\n",
       "6018  Books: anything by Mochael Crichton, Jules Ver...     580\n",
       "6497  I have read like 50 books in my life time that...     185\n",
       "5473  Drunk History, Shameless, Everything is Illumi...     154\n",
       "3067  Shows: Prison BreakMusic: RNB, KpopFood: Spagh...     198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I've pre-wrangled and saved the data, but we'll still need to do a bit more work\n",
    "\n",
    "data = pd.read_csv('favorites_data.csv')\n",
    "del data['Unnamed: 0']\n",
    "\n",
    "# Dataframe info\n",
    "print('Shape:',data.shape)\n",
    "print('Columns:', data.columns)\n",
    "\n",
    "# Sample entries\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7116, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in punctuation:\n",
    "    data['favorites'] = data['favorites'].str.replace(i, ' ')\n",
    "    \n",
    "data['favorites'] = data['favorites'].str.lower()\n",
    "\n",
    "data = data[data.length > 2]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.15, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(max_df=.15, min_df=2)\n",
    "\n",
    "tf.fit(data.favorites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_process(text, tf_stop_words):\n",
    "    \n",
    "    # Tfidf stopwords\n",
    "    cleaned = [word for word in text.split() if word not in tf_stop_words]\n",
    "    \n",
    "    # NLTK stopwords\n",
    "    cleaned = [word for word in cleaned if word not in stopwords.words('english')]\n",
    "    \n",
    "    cleaned = [word for word in cleaned if len(word) > 1]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "data['favorites'] = data['favorites'].apply(text_process, tf_stop_words=tf.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5755    [recently, symphonic, metal, authors, laurel, ...\n",
       "4844    [happiness, intelligent, people, thing, know, ...\n",
       "2860    [basically, thai, indian, mexican, vietnamese,...\n",
       "19      [walking, dead, girl, modern, family, orange, ...\n",
       "4551    [huge, book, nerd, mostly, fiction, biography,...\n",
       "Name: favorites, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['favorites'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('things', 1250),\n",
       " ('big', 1164),\n",
       " ('one', 1152),\n",
       " ('book', 1129),\n",
       " ('many', 1128),\n",
       " ('lot', 1102),\n",
       " ('enjoy', 1100),\n",
       " ('series', 1092),\n",
       " ('everything', 1046),\n",
       " ('black', 1046),\n",
       " ('pretty', 1046),\n",
       " ('reading', 1045),\n",
       " ('movie', 1032),\n",
       " ('dead', 1003),\n",
       " ('thrones', 992),\n",
       " ('life', 988),\n",
       " ('american', 961),\n",
       " ('fiction', 959),\n",
       " ('house', 938),\n",
       " ('always', 933),\n",
       " ('show', 924),\n",
       " ('horror', 908),\n",
       " ('listen', 891),\n",
       " ('list', 857),\n",
       " ('eat', 817),\n",
       " ('country', 812),\n",
       " ('get', 795),\n",
       " ('etc', 792),\n",
       " ('bad', 775),\n",
       " ('harry', 771),\n",
       " ('favorites', 737),\n",
       " ('star', 730),\n",
       " ('go', 714),\n",
       " ('potter', 711),\n",
       " ('know', 704),\n",
       " ('stuff', 694),\n",
       " ('sushi', 688),\n",
       " ('old', 657),\n",
       " ('walking', 650),\n",
       " ('try', 641)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "things_liked = list(data['favorites'])\n",
    "\n",
    "everything_liked = [item for user in things_liked for item in user]\n",
    "\n",
    "results = Counter(everything_liked)\n",
    "results.most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This a bag-of-words approach so it requires some pop-culture domain knowledge to interpret. Here's what I notice in the list:\n",
    "\n",
    "1. Rock music\n",
    "2. Game of Thrones\n",
    "3. The Walking Dead\n",
    "4. Harry Potter\n",
    "5. Sushi\n",
    "6. Italian food\n",
    "7. Pop music\n",
    "8. Thai food\n",
    "9. Mexican food\n",
    "10. Hip-hop music\n",
    " \n",
    "Things that \"might\" be mentioned (in no particular order):\n",
    "1. This American Life\n",
    "2. Black Mirror\n",
    "3. Orange is the New Black\n",
    "4. Clockwork Orange\n",
    "5. American Horror Story\n",
    "6. House, M.D.\n",
    "7. House music\n",
    "8. Star Wars\n",
    "9. Star Trek\n",
    "10. Family Guy\n",
    "11. Modern Family\n",
    "\n",
    "These are my educated guesses. People might be huge fans of *Little House on the Prairie*, but I doubt it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the DataFrame\n",
    "This is where the real magic happens. I iterate over each word used, then (if necessary) create a new column with that word and assign it a value of 1. We can then look at correlations and create a simple recommender system.\n",
    "\n",
    "I saved the results to a csv, then put the code in a function. I only needed to run it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(data)):\n",
    "\n",
    "    for j in things_liked[i]:\n",
    "        if results[j] >= 5: #set a threshold\n",
    "            df.at[i, j] = 1\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('favorites_processed_2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def favorites_df(mentions=20, save=True):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        things_list = []\n",
    "        things_liked = text_process(data.iloc[i]['favorites'], stem=False)\n",
    "        if len(things_liked) > 0:\n",
    "            for j in things_liked:\n",
    "                if results[j] >= mentions: #set a threshold\n",
    "                    df.set_value(i, j, 1)\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    if save == True:\n",
    "        df.to_csv('favorites_processed.csv', encoding='utf-8')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieving the csv I saved with the previous function\n",
    "df = pd.read_csv('favorites_processed.csv')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corrs(corr_item=df['thrones'], df=df):\n",
    "    '''\n",
    "    Finds correlated words, then sorts by absolute value to account for\n",
    "    strong negative correlations.\n",
    "    '''\n",
    "    cor = df.select_dtypes(include=[np.number]).corrwith(corr_item)\n",
    "    df = pd.DataFrame(cor.sort_values(ascending=False),columns=['corr'])\n",
    "    df['absol'] = np.abs(df['corr'])\n",
    "    return df[df.absol < 1].sort_values('absol', ascending=False)['corr']         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Titles that were ambiguous from our top results, or might\n",
    "# otherwise be hard to find. Not perfect, but works well enough.\n",
    "\n",
    "df['always_sunny'] = np.where((df.always == 1) & (df.sunny == 1), 1, 0)\n",
    "df['american_dad'] = np.where((df.american == 1) & (df.dad == 1) & (df.horror == 0), 1, 0)\n",
    "df['american_horror_story'] = np.where((df.american == 1) & (df.dad == 0) & (df.horror == 1), 1, 0)\n",
    "df['big_bang_theory'] = np.where((df.bang == 1) & (df.theory == 1), 1, 0)\n",
    "df['black_mirror'] = np.where((df.black == 1) & (df.mirror == 1), 1, 0)\n",
    "df['clockwork_orange'] = np.where((df.clockwork == 1) & (df.orange == 1), 1, 0)\n",
    "\n",
    "# People love to point out that they hate country music.\n",
    "df['country'] = np.where((df.country == 1) & (df['except'] == 0), 1, 0) \n",
    "\n",
    "df['crazy_ex_girlfriend'] = np.where((df.crazy == 1) & (df.ex == 1), 1, 0)\n",
    "df['criminal_minds'] = np.where((df.criminal == 1) & (df.minds == 1), 1, 0)\n",
    "df['family_guy'] = np.where((df.family == 1) & (df.guy == 1), 1, 0)\n",
    "df['final_fantasy'] = np.where((df.final == 1) & (df.fantasy == 1), 1, 0)\n",
    "df['lord_of_the_rings'] = np.where((df.lord == 1) & (df.rings == 1), 1, 0)\n",
    "df['modern_family'] = np.where((df.modern == 1) & (df.family == 1), 1, 0)\n",
    "df['name_of_the_wind'] = np.where((df.wind == 1) & (df.name == 1), 1, 0) # My favorite book!\n",
    "df['orange_is_the_new_black'] = np.where((df.black == 1) & (df.orange == 1), 1, 0)\n",
    "df['pans_labyrinth'] = np.where((df.pan == 1) & (df.labyrinth == 1), 1, 0)\n",
    "df['princess_bride'] = np.where((df.princess == 1) & (df.bride == 1), 1, 0)\n",
    "df['south_park'] = np.where((df.south == 1) & (df.park == 1), 1, 0)\n",
    "df['star_trek'] = np.where((df.star == 1) & (df.trek == 1), 1, 0) # Really, \"trek\" would be enough.\n",
    "df['star_wars'] = np.where((df.star == 1) & (df.wars == 1), 1, 0)\n",
    "df['the_office'] = np.where((df.office == 1) & (df.space == 0), 1, 0)\n",
    "df['this_american_life'] = np.where((df.american == 1) & (df.life == 1), 1, 0)\n",
    "df['whose_line'] = np.where((df.whose == 1) & (df.line == 1), 1, 0)\n",
    "df['game_of_thrones'] = df.thrones\n",
    "del df['thrones']\n",
    "\n",
    "# One of my favorite shows, and a huge pain to search for!\n",
    "df['lost_tv_show'] = np.where((df.lost == 1) & (df.translation == 0) &\\\n",
    "                              (df.boys == 0) & (df.paradise == 0) &\\\n",
    "                              (df.getting == 0) & (df.world == 0) &\\\n",
    "                              (df.souls == 0) & (df.children == 0) &\\\n",
    "                              (df.girl == 0), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix          0.131429\n",
       "interstellar    0.126217\n",
       "memento         0.111756\n",
       "500             0.104910\n",
       "linkin          0.102768\n",
       "knight          0.099501\n",
       "julian          0.089933\n",
       "fight           0.089708\n",
       "assassin        0.088461\n",
       "club            0.088178\n",
       "Name: corr, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs(df.inception).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People who like Inception also tend to like:\n",
    "- The Matrix\n",
    "- Interstellar\n",
    "- Memento\n",
    "- 500 Days of Summer\n",
    "- Linkin Park\n",
    "- The Dark Knight\n",
    "- Fight Club\n",
    "- Hunting Nemo\n",
    "\n",
    "Wait, that's not the name of the movie! Does \"hunting\" refer to *Good Will Hunting*? Let's make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goodwill     0.209285\n",
       "bourne       0.151121\n",
       "beautiful    0.109954\n",
       "bergerac     0.103301\n",
       "cyrano       0.103301\n",
       "gran         0.103301\n",
       "scotland     0.103301\n",
       "nutshell     0.103301\n",
       "raid         0.103301\n",
       "thirteen     0.103301\n",
       "Name: corr, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs(df['hunting']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear so. It's interesting to note that it's pretty common to simply be a \"Matt Damon\" fan (e.g., Jason Bourne, The Departed).\n",
    "\n",
    "So, what about *Good Will Hunting*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corrs(df['will'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't run it, but this results in a KeyError! The problem with my stopwords is that they sometimes filter out names of titles, such as \"Finding\" Nemo or \"Good\" Will Hunting.\n",
    "\n",
    "However, I'd argue this is more a problem with unstructured data, than the use of stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters\n",
    "What if there are certain \"types\" of people in terms of personal tastes? A cluster analysis could uncover this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe. We need to increase the popularity\n",
    "# threshold to 50 to reduce dimensionality.\n",
    "\n",
    "df_popular = favorites_df(mentions=50, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_popular.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no avoiding it: We'll have to reduce the number of dimensions in order to perform a cluster analysis. Let's see if we can find a point of diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "variance_dict = {}\n",
    "\n",
    "for i in range(1,1100,100):\n",
    "    pca = PCA(n_components=i, whiten=True).fit(df_popular)\n",
    "    variance_dict[i] = sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vdf = pd.DataFrame([variance_dict]).T.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an elbow at approximately 100, but that's still too many features for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variance_dict = {}\n",
    "\n",
    "for i in range(1,210,20):\n",
    "    pca = PCA(n_components=i, whiten=True).fit(df_popular)\n",
    "    variance_dict[i] = sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "pd.DataFrame([variance_dict]).T.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 appears to be another good option. However, after much trial and error, I discovered that it's most effective to capture about 10% of the variance with only 10 features. It makes our clustering decisions much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10, whiten=True).fit(df_popular)\n",
    "\n",
    "dframe = pca.transform(df_popular)\n",
    "\n",
    "print('PCA explained variance:', sum(pca.explained_variance_ratio_))\n",
    "print('Shape:', dframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only catching a tiny fraction of the variance, but it'll have to do.\n",
    "\n",
    "Next question: How many clusters should we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inertia measures the average distance to the nearest centroid.\n",
    "# We want it to be low, but adding more clusters is always going\n",
    "# to reduce it. So we need to find a point of diminishing returns.\n",
    "\n",
    "kdict = {}\n",
    "\n",
    "for i in range(2,20):\n",
    "    clf = KMeans(n_clusters=i)\n",
    "    clf.fit(dframe)\n",
    "    kdict[i] = clf.inertia_\n",
    "\n",
    "kframe = pd.DataFrame([kdict]).T\n",
    "kframe.columns = ['inertia']\n",
    "\n",
    "# Calculate the improvement as a percentage\n",
    "kframe['improvement'] = (kframe['inertia'].shift(1) - kframe['inertia'])/kframe['inertia'].shift(1)\n",
    "\n",
    "kframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kframe.improvement.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 appears to be a good number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=12)\n",
    "clf.fit(df_popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_popular['cluster'] = clf.labels_ + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = df_popular.groupby('cluster').mean()\n",
    "\n",
    "groups = groups[(groups > .1)]\n",
    "groups.dropna(how='all', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = groups.dropna(how='all',axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrs(df.black).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(groups)):\n",
    "    print('Cluster', str(i+1))\n",
    "    for j in groups.columns:\n",
    "        if groups.iloc[i][j] > .2:\n",
    "            print(j, end=', ')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Your Tastes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like Game of Thrones, Breaking Bad, Lost, Ferris Bueller, and The Walking Dead. Would I like Margaret Atwood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_tastes = {'game_of_thrones': 1, 'breaking': 1, 'bad': 1, 'walking': 1, 'dead': 1,\n",
    "             'lost_tv_show': 1, 'name_of_the_wind': 1, 'ferris': 1, 'bueller': 1}\n",
    "\n",
    "df = df.append(my_tastes, ignore_index=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = df.copy()\n",
    "y = x.pop('atwood')\n",
    "\n",
    "del x['handmaid']\n",
    "del x['margaret']\n",
    "del x['tale']\n",
    "del x['heart']\n",
    "del x['goes']\n",
    "del x['last']\n",
    "del x['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=.20)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=500) # We need a lot of estimators to create precise predictions here.\n",
    "clf.fit(xtrain, ytrain)\n",
    "pred = clf.predict(xtest)\n",
    "pred_proba = clf.predict_proba(xtest)\n",
    "\n",
    "accuracy_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem! There's a huge class imbalance, and the model is going to be 99% accurate just by predicting that no one likes Margaret Atwood.\n",
    "\n",
    "I'm quite willing to have some false positives, so let's create a custom set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "atwood_average = pred_proba[:,1].mean()\n",
    "atwood_std = pred_proba[:,1].std()\n",
    "\n",
    "print(\"Average probability of liking Margaret Atwood:\", atwood_average)\n",
    "print(\"Standard deviation:\", atwood_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine them, and say that if your probability is one standard deviation above the average, then you're predicted to like her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_to_like = atwood_average + atwood_std\n",
    "pred_custom = []\n",
    "\n",
    "for i in pred_proba[:,1]:\n",
    "    if i >= predicted_to_like:\n",
    "        pred_custom.append(1)\n",
    "    else:\n",
    "        pred_custom.append(0)\n",
    "\n",
    "np.sum(pred_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Now many people are predicted to like her. Let's evaluate this now using recall score, rather than accuracy. This benchmark is more appropriate, considering I'm okay with false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall_score(ytest, pred_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare it to my original predictions. \n",
    "recall_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "atwood = pd.DataFrame([df.columns, clf.feature_importances_, ]).transpose()\n",
    "atwood.columns = ['feature', 'importance']\n",
    "\n",
    "atwood.sort_values('importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went off on a tangent. The original question was whether I'd like Margaret Atwood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_me = clf.predict_proba(x.iloc[-1].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_me[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently Margaret Atwood isn't for me. In fact, the model was bold enough to assign a 0% probability! Which makes me wonder -- is it assigning binary labels to everyone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(x)[:,1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, it just really thinks I won't like her. Thanks, model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This doesn't work very well.\n",
    "# It just identifies things that are popular\n",
    "\n",
    "def similar(item, df=df):\n",
    "    similarity_dict = {}\n",
    "    cnt = df[item].sum()\n",
    "    \n",
    "    for i in df.columns:\n",
    "        cnt_i = df[i].sum()\n",
    "        similarity = cnt_i/cnt\n",
    "        similarity_dict[i] = similarity\n",
    "        \n",
    "    df_sim = pd.DataFrame([similarity_dict]).transpose()\n",
    "    df_sim.columns = ['similarity']\n",
    "    return df_sim.sort_values('similarity', ascending=False)\n",
    "        \n",
    "similar('kanye')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
